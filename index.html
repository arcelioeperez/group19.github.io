<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>10. Clustering Lab</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Group 19</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="about.html">10 PCA Lab</a>
</li>
<li>
  <a href="index.html">Applied Exercise - 8</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">10. Clustering Lab</h1>

</div>


<div id="objective" class="section level2">
<h2>Objective</h2>
<p>We are attempting to identify subgroups in the data, also known as clusters.</p>
</div>
<div id="dataset-background" class="section level2">
<h2>Dataset Background</h2>
<p>In this lab, we use a simulated dataset. The data creation process is engineered to create a dichotomy in the data; this dichotomy manifests itself as a shift between the mean of the first half of the observations and the second. This is achieved by adding and subtracting a constant from each half of the dataset. We use the rnorm() function to generate data:</p>
<pre class="r"><code>set.seed(2)
x &lt;- matrix(rnorm(50*2), ncol=2)
x[1:25,1] &lt;- x[1:25,1] +3
x[1:25,2] &lt;- x[1:25,2] -4</code></pre>
</div>
<div id="dataset-exploration-variable-transformation" class="section level2">
<h2>3/4. Dataset Exploration, Variable Transformation</h2>
<p>The lab does not feature any dataset exploration. However, for the sake of illustrating the existing divide between the two groups of data, here is a scatter plot:</p>
<pre class="r"><code>plot(x)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>There is no variable transformation applied for the first half of this lab, but for hierarchical clustering we will be using variable scaling later.</p>
</div>
<div id="running-the-kmeans-function---k-means-clustering" class="section level2">
<h2>5. Running the kmeans() Function - K-Means Clustering</h2>
<p>We now perform K-means clustering with K=2, which means that we attempt to divide the dataset in 2 distinct, non-overlapping groups, or clusters. For this we use the kmeans() function:</p>
<pre class="r"><code>km.out &lt;- kmeans(x,2,nstart=20)</code></pre>
<p>Examining the output of the function:</p>
<pre class="r"><code>(names(km.out))</code></pre>
<pre><code>## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;    
## [5] &quot;tot.withinss&quot; &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;        
## [9] &quot;ifault&quot;</code></pre>
<p>One of the important output elements is “cluster”, which contains the group assignments for each data point:</p>
<pre class="r"><code>(km.out$cluster) </code></pre>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2
## [36] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</code></pre>
<p>We can also plot the same scatter plot as earlier, except this time we color the data points according to their cluster assignment: plot(x,col=(km.out$cluster+1),main=“K-Means Clustering Results with K=2”,xlab="“,ylab=”",pch=20,cex=2) We can see that the function divides the data in two clearly separated groups.</p>
<p>Here we chose K = 2 because we were previously aware that there were two groups in the data we created. However with an unknown dataset, we do not know how many groups to expect. If we had tried to split the data in 3 groups:</p>
<pre class="r"><code>set.seed(4)
km.out &lt;- kmeans(x,3,nstart=20)
(km.out)</code></pre>
<pre><code>## K-means clustering with 3 clusters of sizes 17, 23, 10
## 
## Cluster means:
##         [,1]        [,2]
## 1  3.7789567 -4.56200798
## 2 -0.3820397 -0.08740753
## 3  2.3001545 -2.69622023
## 
## Clustering vector:
##  [1] 1 3 1 3 1 1 1 3 1 3 1 3 1 3 1 3 1 1 1 1 1 3 1 1 1 2 2 2 2 2 2 2 2 2 2
## [36] 2 2 2 2 2 2 2 2 3 2 3 2 2 2 2
## 
## Within cluster sum of squares by cluster:
## [1] 25.74089 52.67700 19.56137
##  (between_SS / total_SS =  79.3 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;    
## [5] &quot;tot.withinss&quot; &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;        
## [9] &quot;ifault&quot;</code></pre>
<p>Notice the use of the nstart argument in the function above. nstart is used to start the clustering algorithm at multiple data points chosen at random, and to then only report the best result found. This method is meant to prevent the clustering algorithm from getting stuck in a local optimum. The clustering algorithm seeks to minimize the total sum of squares within clusters. This number can be accessed from the kmeans() output as an object called tot.withinss:</p>
<pre class="r"><code>set.seed(3)
km.out &lt;- kmeans(x,3,nstart=1)
(km.out$tot.withinss)</code></pre>
<pre><code>## [1] 97.97927</code></pre>
<pre class="r"><code>km.out &lt;- kmeans(x,3,nstart=20)
(km.out$tot.withinss)</code></pre>
<pre><code>## [1] 97.97927</code></pre>
<p>Here we see that both nstart values return the same total sum of squares within clusters. However, it is recommended to use a large nstart value to prevent getting stuck in the wrong optimum when using datasets that aren’t as clearly delimited as this one.</p>
</div>
<div id="running-the-hclust-function---hierarchical-clustering" class="section level2">
<h2>6 Running the hclust() Function - Hierarchical Clustering</h2>
<p>The first thing we will need to work with hierarchical clustering is an Euclidian distance matrix, which will provide the euclidian distance between all the 50 data points. This is necessary, since our way of measuring how dissimilar two data points are, and thus which category they belong to, is based on their relative Euclidian distance:</p>
<pre class="r"><code>dist_matrix &lt;- dist(x)</code></pre>
<p>Now we can use hclust() with that matrix as an argument:</p>
<pre class="r"><code>hc.complete &lt;- hclust(dist_matrix,method=&quot;complete&quot;) </code></pre>
<p>Note that the method = “complete” argument in the formula call refers to complete linkage method used to plot the hierarchical clustering dendrogram. We can also use the alternate methods of single &amp; average linkage by setting:</p>
<pre class="r"><code>hc.average &lt;- hclust(dist_matrix, method=&quot;average&quot;)
hc.single &lt;- hclust(dist_matrix,method=&quot;single&quot;)</code></pre>
<p>We can now plot the dendrograms for all 3 methods using those outputs:</p>
<pre class="r"><code>par(mfrow=c(1,3))
plot(hc.complete, main=&quot;Complete Linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;, cex=0.9)
plot(hc.average, main=&quot;Average Linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;, cex=0.9)
plot(hc.single, main=&quot;Single Linkage&quot;, xlab=&quot;&quot;, sub =&quot;&quot;, cex=0.9)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>And from the hclust() outputs form before, we can obtain the clustering labels per observation associated with a given cut of the dendrogram by using the cutree() function. We give the function the 2 in the second argument to specify we want 2 clusters:</p>
<pre class="r"><code>cutree(hc.complete,2)</code></pre>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2
## [36] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</code></pre>
<pre class="r"><code>cutree(hc.average,2)</code></pre>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 2 2
## [36] 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2</code></pre>
<pre class="r"><code>cutree(hc.single,2)</code></pre>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</code></pre>
<p>We see that the complete &amp; average linkage methods do a good enough job at separating the data, but the single linkage method performs poorly.</p>
<p>When specifying 4 clusters, the split is more spread out between the 1st and 3rd cluster, but we still get only two observations falling into the 2nd and 4th categories, respectively. cutree(hc.single,4)</p>
<pre class="r"><code>cutree(hc.single,4)</code></pre>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3
## [36] 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3</code></pre>
</div>
<div id="additional-notes---scaling-as.dist" class="section level2">
<h2>Additional notes - scaling &amp; as.dist()</h2>
<p>Variable Scaling - it might be necessary to scale variables before performing hierarchical clustering.</p>
<pre class="r"><code>xsc &lt;- scale(x)
plot(hclust(dist(xsc),method=&quot;complete&quot;),main=&quot;Hierarchical Clustering with Scaled Features&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Using the as.dist() function: as.dist() can be used to compute correlation-based distance, and allows to go from a square symmetric matrix to a distance matrix. This matrix can then be passed to the hclust() function:</p>
<pre class="r"><code>x &lt;- matrix (rnorm (30*3) , ncol =3)
dd=as.dist(1- cor(t(x)))
plot(hclust(dd,method=&quot;complete&quot;),
     main=&quot; Complete Linkage with Correlation -Based Distance&quot;,xlab=&quot;&quot;, sub =&quot;&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
